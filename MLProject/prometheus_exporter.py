"""
Prometheus Exporter for ME/CFS vs Depression Model Monitoring
ADVANCE Level - Kriteria 4: Monitoring & Logging

This script exports 10+ custom metrics for model performance monitoring.
Metrics include prediction counts, latency, model confidence, and system health.
"""

import os
import time
import pickle
import numpy as np
import pandas as pd
from flask import Flask, request, jsonify
from prometheus_client import Counter, Histogram, Gauge, generate_latest, REGISTRY
from prometheus_client.core import CollectorRegistry
import mlflow
from datetime import datetime

# Initialize Flask app
app = Flask(__name__)

# Load model
MODEL_PATH = os.getenv('MODEL_PATH', 'model_deploy')
model = None

try:
    model = mlflow.pyfunc.load_model(MODEL_PATH)
    print(f"‚úÖ Model loaded successfully from {MODEL_PATH}")
except Exception as e:
    print(f"‚ùå Error loading model: {e}")

# ============================================================================
# PROMETHEUS METRICS DEFINITION (10+ metrics for ADVANCE level)
# ============================================================================

# 1. Request counter - Total number of prediction requests
prediction_requests_total = Counter(
    'model_prediction_requests_total',
    'Total number of prediction requests',
    ['class_predicted']
)

# 2. Prediction latency histogram - Response time distribution
prediction_latency_seconds = Histogram(
    'model_prediction_latency_seconds',
    'Latency of prediction requests in seconds',
    buckets=[0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0]
)

# 3. Error counter - Failed predictions
prediction_errors_total = Counter(
    'model_prediction_errors_total',
    'Total number of failed predictions',
    ['error_type']
)

# 4. Model confidence gauge - Average prediction confidence
model_confidence_gauge = Gauge(
    'model_prediction_confidence',
    'Average confidence score of predictions'
)

# 5. Prediction distribution - Count per class
class_prediction_counter = Counter(
    'model_class_predictions_total',
    'Total predictions per class',
    ['class_name']
)

# 6. Input feature statistics - Mean value tracker
input_feature_mean = Gauge(
    'model_input_feature_mean',
    'Mean value of input features',
    ['feature_name']
)

# 7. Active requests gauge - Concurrent requests
active_requests_gauge = Gauge(
    'model_active_requests',
    'Number of currently active prediction requests'
)

# 8. Model uptime - Time since model was loaded
model_uptime_seconds = Gauge(
    'model_uptime_seconds',
    'Time in seconds since model was loaded'
)

# 9. Prediction throughput - Requests per minute
prediction_throughput_gauge = Gauge(
    'model_prediction_throughput_rpm',
    'Prediction throughput in requests per minute'
)

# 10. Memory usage gauge - Model memory footprint
model_memory_usage_mb = Gauge(
    'model_memory_usage_megabytes',
    'Model memory usage in megabytes'
)

# 11. Batch size histogram - Distribution of batch sizes
batch_size_histogram = Histogram(
    'model_batch_size',
    'Distribution of prediction batch sizes',
    buckets=[1, 5, 10, 20, 50, 100]
)

# 12. Health status gauge - Model health indicator (1=healthy, 0=unhealthy)
model_health_gauge = Gauge(
    'model_health_status',
    'Model health status (1=healthy, 0=unhealthy)'
)

# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

# Class names mapping
CLASS_NAMES = {0: 'Healthy', 1: 'ME_CFS', 2: 'Depression'}

# Model load time for uptime calculation
MODEL_LOAD_TIME = time.time()

# Throughput tracking
request_timestamps = []

def update_throughput():
    """Update throughput metric based on recent requests"""
    global request_timestamps
    current_time = time.time()
    
    # Keep only requests from last 60 seconds
    request_timestamps = [t for t in request_timestamps if current_time - t < 60]
    
    # Calculate requests per minute
    rpm = len(request_timestamps)
    prediction_throughput_gauge.set(rpm)

def update_uptime():
    """Update model uptime metric"""
    uptime = time.time() - MODEL_LOAD_TIME
    model_uptime_seconds.set(uptime)

def estimate_memory_usage():
    """Estimate model memory usage"""
    try:
        import psutil
        process = psutil.Process(os.getpid())
        memory_mb = process.memory_info().rss / (1024 * 1024)
        model_memory_usage_mb.set(memory_mb)
    except Exception as e:
        print(f"Warning: Could not estimate memory usage: {e}")

def update_health_status():
    """Check and update model health status"""
    try:
        if model is not None:
            model_health_gauge.set(1)
        else:
            model_health_gauge.set(0)
    except Exception:
        model_health_gauge.set(0)

# ============================================================================
# API ENDPOINTS
# ============================================================================

@app.route('/invocations', methods=['POST'])
def predict():
    """
    Main prediction endpoint compatible with MLflow serving format
    
    Expected input: JSON with 'dataframe_split' containing 15 features
    Returns: Predictions array
    """
    start_time = time.time()
    
    # Increment active requests
    active_requests_gauge.inc()
    
    try:
        # Parse input data
        data = request.get_json()
        
        if not data or 'dataframe_split' not in data:
            prediction_errors_total.labels(error_type='invalid_input').inc()
            active_requests_gauge.dec()
            return jsonify({'error': 'Invalid input format. Expected dataframe_split'}), 400
        
        # Convert to DataFrame
        df_data = data['dataframe_split']
        df = pd.DataFrame(df_data['data'], columns=df_data['columns'])
        
        # Record batch size
        batch_size = len(df)
        batch_size_histogram.observe(batch_size)
        
        # Track input feature statistics
        for col in df.columns:
            mean_val = df[col].mean()
            input_feature_mean.labels(feature_name=col).set(mean_val)
        
        # Make predictions
        predictions = model.predict(df)
        
        # Calculate prediction confidence (using a simple metric)
        # In real scenario, you might extract probability scores
        confidence = 0.995  # Using training accuracy as baseline confidence
        model_confidence_gauge.set(confidence)
        
        # Update metrics per prediction
        for pred in predictions:
            class_name = CLASS_NAMES.get(int(pred), 'Unknown')
            prediction_requests_total.labels(class_predicted=str(pred)).inc()
            class_prediction_counter.labels(class_name=class_name).inc()
        
        # Record latency
        latency = time.time() - start_time
        prediction_latency_seconds.observe(latency)
        
        # Update throughput
        request_timestamps.append(time.time())
        update_throughput()
        
        # Update system metrics
        update_uptime()
        estimate_memory_usage()
        update_health_status()
        
        # Decrement active requests
        active_requests_gauge.dec()
        
        return jsonify({
            'predictions': predictions.tolist(),
            'latency_seconds': latency,
            'batch_size': batch_size
        })
    
    except Exception as e:
        prediction_errors_total.labels(error_type=str(type(e).__name__)).inc()
        active_requests_gauge.dec()
        return jsonify({'error': str(e)}), 500

@app.route('/metrics', methods=['GET'])
def metrics():
    """
    Prometheus metrics endpoint
    Returns metrics in Prometheus exposition format
    """
    # Update system metrics before serving
    update_uptime()
    estimate_memory_usage()
    update_health_status()
    update_throughput()
    
    return generate_latest(REGISTRY), 200, {'Content-Type': 'text/plain; charset=utf-8'}

@app.route('/health', methods=['GET'])
def health():
    """Health check endpoint"""
    update_health_status()
    
    if model is not None:
        return jsonify({
            'status': 'healthy',
            'model_loaded': True,
            'uptime_seconds': time.time() - MODEL_LOAD_TIME,
            'timestamp': datetime.now().isoformat()
        }), 200
    else:
        return jsonify({
            'status': 'unhealthy',
            'model_loaded': False,
            'timestamp': datetime.now().isoformat()
        }), 503

@app.route('/', methods=['GET'])
def index():
    """Root endpoint with API information"""
    return jsonify({
        'service': 'ME/CFS vs Depression Model',
        'version': '1.0.0',
        'endpoints': {
            '/invocations': 'POST - Make predictions',
            '/metrics': 'GET - Prometheus metrics',
            '/health': 'GET - Health check'
        },
        'monitoring': 'Prometheus + Grafana',
        'status': 'ADVANCE - Kriteria 4'
    })

# ============================================================================
# MAIN
# ============================================================================

if __name__ == '__main__':
    print("=" * 70)
    print("üöÄ ME/CFS vs Depression Model - Prometheus Monitoring")
    print("=" * 70)
    print(f"üìä Metrics endpoint: http://localhost:5000/metrics")
    print(f"üè• Health endpoint: http://localhost:5000/health")
    print(f"üîÆ Prediction endpoint: http://localhost:5000/invocations")
    print(f"‚úÖ Total metrics exported: 12+")
    print("=" * 70)
    
    # Initialize health status
    update_health_status()
    
    # Run Flask app
    app.run(host='0.0.0.0', port=5000, debug=False)
